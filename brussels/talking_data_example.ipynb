{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TalkingData Mobile User Demographics - 3rd place solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition, Kagglers are challenged to build a model predicting usersâ€™ demographic characteristics based on their app usage, geolocation, and mobile device properties. Doing so will help millions of developers and brand advertisers around the world pursue data-driven marketing efforts which are relevant to their users and catered to their preferences.\n",
    "\n",
    "https://www.kaggle.com/c/talkingdata-mobile-user-demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gender_age_train.csv, gender_age_test.csv -- the training and test set\n",
    "\n",
    "Variable to predict: Group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (74645, 3), test shape: (112071, 0)\n",
      "phone shape (186716, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events shape: (3252950, 4), appevents shape: (32473067, 3), applabels shape: (459943, 2), labels shape: (459943, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [13, 7]\n",
    "\n",
    "datadir = '../input'\n",
    "gatrain = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'), index_col='device_id')\n",
    "gatest = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'), index_col = 'device_id')\n",
    "print('train shape: {}, test shape: {}'.format(gatrain.shape, gatest.shape))\n",
    "\n",
    "phone = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'))\n",
    "phone = phone.drop_duplicates('device_id',keep='first').set_index('device_id')\n",
    "print('phone shape', phone.shape)\n",
    "\n",
    "events = pd.read_csv(os.path.join(datadir,'events.csv'),  parse_dates=['timestamp'], index_col='event_id')\n",
    "appevents = pd.read_csv(os.path.join(datadir,'app_events.csv'), usecols=['event_id','app_id','is_active'], dtype={'is_active':bool})\n",
    "applabels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\n",
    "labelcategories = pd.read_csv(os.path.join(datadir, 'label_categories.csv'))\n",
    "print('events shape: {}, appevents shape: {}, applabels shape: {}, labels shape: {}'.format(events.shape, appevents.shape, applabels.shape, applabels.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KEY: We used different models for these 2 groups**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding with sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of apps in devices: (32473067, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<74645x20836 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 990277 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def get_sparse_data(train, test, col):\n",
    "    \"\"\" A sparse matrix of features can be constructed using the csr_matrix constructor:\n",
    "        csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])\n",
    "        where \"data\", \"row_ind\" and \"col_ind\" satisfy the\n",
    "        relationship \"a[row_ind[k], col_ind[k]] = data[k]\"\"\n",
    "    \"\"\"\n",
    "    full = pd.concat([train[col], test[col]], axis=0)\n",
    "    full = full.fillna(-9999)\n",
    "    appencoder = LabelEncoder().fit(full)\n",
    "    train[col] = appencoder.transform(train[col])\n",
    "    test[col] = appencoder.transform(test[col])\n",
    "    nvalues = len(appencoder.classes_)\n",
    "\n",
    "    xtr = csr_matrix(\n",
    "        (np.ones(len(train)), (train.trainrow, train[col])),\n",
    "        shape=(train.shape[0], nvalues))\n",
    "\n",
    "    xte = csr_matrix(\n",
    "        (np.ones(len(test)), (test.testrow, test[col])),\n",
    "        shape=(test.shape[0], nvalues))\n",
    "    assert np.sum(xtr) == len(train)\n",
    "    assert np.sum(xte) == len(test)\n",
    "    return xtr, xte\n",
    "\n",
    "def get_sparse_from_grouped(train, test, trans, col):\n",
    "    \"\"\" \n",
    "    Example:\n",
    "    Remember that there are many events for each device.\n",
    "    for each device I want to mark which event id was purchased with\n",
    "    I'll have as many event values columns as events are linked to devices    \n",
    "    \"\"\"\n",
    "    trans[col] = trans[col].fillna(-9999)\n",
    "    appencoder = LabelEncoder().fit(trans[col])\n",
    "    trans[col] = appencoder.transform(trans[col])\n",
    "    nvalues = len(appencoder.classes_)\n",
    "\n",
    "    aggtable = (\n",
    "        trans.groupby(['device_id', col])['device_id']\n",
    "        .agg(['count'])\n",
    "        .merge(train[['trainrow']], how='left', left_index=True, right_index=True)\n",
    "        .merge(test[['testrow']], how='left', left_index=True, right_index=True)\n",
    "        .reset_index()\n",
    "    )\n",
    "    temp = aggtable.dropna(subset=['trainrow'])\n",
    "    xtr = csr_matrix(\n",
    "        (np.ones(len(temp)).astype(float), (temp.trainrow, temp[col])),\n",
    "        shape=(train.shape[0], nvalues))\n",
    "\n",
    "    temp = aggtable.dropna(subset=['testrow'])\n",
    "    xte = csr_matrix(\n",
    "        (np.ones(len(temp)).astype(float), (temp.testrow, temp[col])),\n",
    "        shape=(test.shape[0], nvalues))\n",
    "    assert xtr.shape[0] == len(train)\n",
    "    assert xte.shape[0] == len(test)\n",
    "    return xtr, xte\n",
    "\n",
    "# Simple:\n",
    "gatrain[\"trainrow\"] = np.arange(gatrain.shape[0])\n",
    "gatest[\"testrow\"] = np.arange(gatest.shape[0])\n",
    "\n",
    "gatrain['model'] = phone[\"device_model\"]\n",
    "gatest['model'] = phone[\"device_model\"]\n",
    "xtrain, xtest = get_sparse_data(gatrain, gatest, 'model')\n",
    "\n",
    "# Grouped: \n",
    "transactions = appevents.merge(\n",
    "    events[[\"device_id\"]], how=\"left\", left_on=\"event_id\", right_index=True\n",
    ")\n",
    "print('shape of apps in devices: {}'.format(transactions.shape))\n",
    "xtrain_grouped, xtest_grouped = get_sparse_from_grouped(gatrain, gatest, transactions, 'app_id')\n",
    "\n",
    "xtrain = hstack([xtrain, xtrain_grouped]).tocsr()\n",
    "xtest = hstack([xtest, xtest_grouped]).tocsr()\n",
    "xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of brands and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each APP, we list all the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<74645x2045 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1878613 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# app_lab = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\n",
    "# app_lab = app_lab.groupby(\"app_id\")[\"label_id\"].apply(\n",
    "#     lambda x: \" \".join(str(s) for s in x)\n",
    "# )\n",
    "# appevents[\"app_lab\"] = appevents[\"app_id\"].map(app_lab)\n",
    "# appevents = appevents.groupby(\"event_id\")[\"app_lab\"].apply(\n",
    "#     lambda x: \" \".join(str(s) for s in x)\n",
    "# )\n",
    "# events[\"app_lab\"] = events.index.map(appevents)\n",
    "# events = events.groupby(\"device_id\")[\"app_lab\"].apply(\n",
    "#     lambda x: \" \".join(str(s) for s in x)\n",
    "# )\n",
    "# gatrain[\"app_lab\"] = gatrain.index.map(events)\n",
    "# gatest[\"app_lab\"] = gatest.index.map(events)\n",
    "\n",
    "# gatrain['device_model'] = phone['device_model']\n",
    "# gatrain['phone_brand'] = phone['phone_brand']\n",
    "\n",
    "# gatest['device_model'] = phone['device_model']\n",
    "# gatest['phone_brand'] = phone['phone_brand']\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# def get_hash_data(train, test):\n",
    "#     df = pd.concat((train, test), axis=0, ignore_index=True, sort=True)\n",
    "#     split_len = len(train)\n",
    "#     tfv = TfidfVectorizer(min_df=1)\n",
    "#     df = (\n",
    "#         df[[\"phone_brand\", \"device_model\", \"app_lab\"]]\n",
    "#         .astype(np.str)\n",
    "#         .apply(lambda x: \" \".join(s for s in x), axis=1)\n",
    "#         .fillna(\"Missing\")\n",
    "#     )\n",
    "#     df_tfv = tfv.fit_transform(df)\n",
    "#     train = df_tfv[:split_len, :]\n",
    "#     test = df_tfv[split_len:, :]\n",
    "#     return train, test\n",
    "\n",
    "# xtrain_bag, xtest_bag = get_hash_data(gatrain, gatest)\n",
    "# xtrain_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting NNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training Logistic and Keras. Using 5 folds, bagged 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(40, activation=\"tanh\", kernel_initializer=\"normal\")`\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"softmax\", kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic logloss for fold 1 is 2.286249574007255\n",
      "Epoch 1/20\n",
      "74645/74645 [==============================] - 11s 150us/step - loss: 2.4356 - acc: 0.1271\n",
      "Epoch 2/20\n",
      "74645/74645 [==============================] - 10s 135us/step - loss: 2.3915 - acc: 0.1471\n",
      "Epoch 3/20\n",
      "74645/74645 [==============================] - 10s 139us/step - loss: 2.3495 - acc: 0.1652\n",
      "Epoch 4/20\n",
      "74645/74645 [==============================] - 10s 139us/step - loss: 2.3268 - acc: 0.1737\n",
      "Epoch 5/20\n",
      "74645/74645 [==============================] - 11s 141us/step - loss: 2.3133 - acc: 0.1784\n",
      "Epoch 6/20\n",
      "74645/74645 [==============================] - 11s 148us/step - loss: 2.3041 - acc: 0.1821\n",
      "Epoch 7/20\n",
      "74645/74645 [==============================] - 10s 137us/step - loss: 2.2940 - acc: 0.1868\n",
      "Epoch 8/20\n",
      "74645/74645 [==============================] - 10s 139us/step - loss: 2.2857 - acc: 0.1906\n",
      "Epoch 9/20\n",
      "74645/74645 [==============================] - 11s 142us/step - loss: 2.2789 - acc: 0.1949\n",
      "Epoch 10/20\n",
      "74645/74645 [==============================] - 11s 146us/step - loss: 2.2738 - acc: 0.1957\n",
      "Epoch 11/20\n",
      "74645/74645 [==============================] - 11s 142us/step - loss: 2.2687 - acc: 0.1964\n",
      "Epoch 12/20\n",
      "74645/74645 [==============================] - 11s 142us/step - loss: 2.2654 - acc: 0.1986\n",
      "Epoch 13/20\n",
      "74645/74645 [==============================] - 11s 141us/step - loss: 2.2607 - acc: 0.1984\n",
      "Epoch 14/20\n",
      "74645/74645 [==============================] - 10s 141us/step - loss: 2.2561 - acc: 0.2018\n",
      "Epoch 15/20\n",
      "74645/74645 [==============================] - 10s 139us/step - loss: 2.2509 - acc: 0.2056\n",
      "Epoch 16/20\n",
      "74645/74645 [==============================] - 10s 140us/step - loss: 2.2479 - acc: 0.2059\n",
      "Epoch 17/20\n",
      "74645/74645 [==============================] - 10s 138us/step - loss: 2.2429 - acc: 0.2083\n",
      "Epoch 18/20\n",
      "74645/74645 [==============================] - 10s 139us/step - loss: 2.2393 - acc: 0.2065\n",
      "Epoch 19/20\n",
      "74645/74645 [==============================] - 10s 140us/step - loss: 2.2377 - acc: 0.2089\n",
      "Epoch 20/20\n",
      "74645/74645 [==============================] - 11s 143us/step - loss: 2.2294 - acc: 0.2105\n",
      "Total: Keras-n/e logloss for fold 1 is 2.1436058350843448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(40, activation=\"tanh\", kernel_initializer=\"normal\")`\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"softmax\", kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic logloss for fold 2 is 2.292489002525951\n",
      "Epoch 1/20\n",
      "74645/74645 [==============================] - 10s 137us/step - loss: 2.4366 - acc: 0.1315\n",
      "Epoch 2/20\n",
      "74645/74645 [==============================] - 10s 128us/step - loss: 2.3848 - acc: 0.1498\n",
      "Epoch 3/20\n",
      "74645/74645 [==============================] - 10s 129us/step - loss: 2.3455 - acc: 0.1664\n",
      "Epoch 4/20\n",
      "74645/74645 [==============================] - 10s 140us/step - loss: 2.3263 - acc: 0.1732\n",
      "Epoch 5/20\n",
      "74645/74645 [==============================] - 10s 134us/step - loss: 2.3125 - acc: 0.1787\n",
      "Epoch 6/20\n",
      "74645/74645 [==============================] - 10s 134us/step - loss: 2.3003 - acc: 0.1852\n",
      "Epoch 7/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.2935 - acc: 0.1864\n",
      "Epoch 8/20\n",
      "74645/74645 [==============================] - 9s 126us/step - loss: 2.2882 - acc: 0.1877\n",
      "Epoch 9/20\n",
      "74645/74645 [==============================] - 9s 124us/step - loss: 2.2824 - acc: 0.1922\n",
      "Epoch 10/20\n",
      "74645/74645 [==============================] - 9s 125us/step - loss: 2.2741 - acc: 0.1943\n",
      "Epoch 11/20\n",
      "74645/74645 [==============================] - 9s 125us/step - loss: 2.2692 - acc: 0.1969\n",
      "Epoch 12/20\n",
      "74645/74645 [==============================] - 10s 133us/step - loss: 2.2640 - acc: 0.1981\n",
      "Epoch 13/20\n",
      "74645/74645 [==============================] - 10s 132us/step - loss: 2.2609 - acc: 0.1985\n",
      "Epoch 14/20\n",
      "74645/74645 [==============================] - 9s 126us/step - loss: 2.2576 - acc: 0.2007\n",
      "Epoch 15/20\n",
      "74645/74645 [==============================] - 10s 128us/step - loss: 2.2520 - acc: 0.2030\n",
      "Epoch 16/20\n",
      "74645/74645 [==============================] - 10s 131us/step - loss: 2.2503 - acc: 0.2029\n",
      "Epoch 17/20\n",
      "74645/74645 [==============================] - 9s 127us/step - loss: 2.2441 - acc: 0.2056\n",
      "Epoch 18/20\n",
      "74645/74645 [==============================] - 10s 128us/step - loss: 2.2405 - acc: 0.2073\n",
      "Epoch 19/20\n",
      "74645/74645 [==============================] - 10s 128us/step - loss: 2.2367 - acc: 0.2082\n",
      "Epoch 20/20\n",
      "74645/74645 [==============================] - 9s 127us/step - loss: 2.2303 - acc: 0.2113\n",
      "Total: Keras-n/e logloss for fold 2 is 2.1594185256838085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(40, activation=\"tanh\", kernel_initializer=\"normal\")`\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"softmax\", kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic logloss for fold 3 is 2.2864477966004864\n",
      "Epoch 1/20\n",
      "74645/74645 [==============================] - 10s 133us/step - loss: 2.4371 - acc: 0.1312\n",
      "Epoch 2/20\n",
      "74645/74645 [==============================] - 9s 126us/step - loss: 2.3859 - acc: 0.1486\n",
      "Epoch 3/20\n",
      "74645/74645 [==============================] - 9s 125us/step - loss: 2.3458 - acc: 0.1672\n",
      "Epoch 4/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.3264 - acc: 0.1753\n",
      "Epoch 5/20\n",
      "74645/74645 [==============================] - 10s 131us/step - loss: 2.3134 - acc: 0.1783\n",
      "Epoch 6/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.3027 - acc: 0.1843\n",
      "Epoch 7/20\n",
      "74645/74645 [==============================] - 10s 134us/step - loss: 2.2947 - acc: 0.1872\n",
      "Epoch 8/20\n",
      "74645/74645 [==============================] - 10s 133us/step - loss: 2.2875 - acc: 0.1896\n",
      "Epoch 9/20\n",
      "74645/74645 [==============================] - 10s 133us/step - loss: 2.2828 - acc: 0.1910\n",
      "Epoch 10/20\n",
      "74645/74645 [==============================] - 10s 132us/step - loss: 2.2759 - acc: 0.1936\n",
      "Epoch 11/20\n",
      "74645/74645 [==============================] - 10s 134us/step - loss: 2.2715 - acc: 0.1950\n",
      "Epoch 12/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.2674 - acc: 0.1970\n",
      "Epoch 13/20\n",
      "74645/74645 [==============================] - 10s 129us/step - loss: 2.2606 - acc: 0.1992\n",
      "Epoch 14/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.2558 - acc: 0.2017\n",
      "Epoch 15/20\n",
      "74645/74645 [==============================] - 10s 127us/step - loss: 2.2535 - acc: 0.2029\n",
      "Epoch 16/20\n",
      "74645/74645 [==============================] - 10s 127us/step - loss: 2.2476 - acc: 0.2058\n",
      "Epoch 17/20\n",
      "74645/74645 [==============================] - 10s 128us/step - loss: 2.2438 - acc: 0.2068\n",
      "Epoch 18/20\n",
      "74645/74645 [==============================] - 10s 132us/step - loss: 2.2391 - acc: 0.2063\n",
      "Epoch 19/20\n",
      "74645/74645 [==============================] - 10s 139us/step - loss: 2.2355 - acc: 0.2095\n",
      "Epoch 20/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.2330 - acc: 0.2076\n",
      "Total: Keras-n/e logloss for fold 3 is 2.1561983220452694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(40, activation=\"tanh\", kernel_initializer=\"normal\")`\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"softmax\", kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic logloss for fold 4 is 2.2937916211011347\n",
      "Epoch 1/20\n",
      "74645/74645 [==============================] - 10s 137us/step - loss: 2.4362 - acc: 0.1313\n",
      "Epoch 2/20\n",
      "74645/74645 [==============================] - 10s 129us/step - loss: 2.3804 - acc: 0.1518\n",
      "Epoch 3/20\n",
      "74645/74645 [==============================] - 10s 128us/step - loss: 2.3444 - acc: 0.1668\n",
      "Epoch 4/20\n",
      "74645/74645 [==============================] - 10s 133us/step - loss: 2.3244 - acc: 0.1736\n",
      "Epoch 5/20\n",
      "74645/74645 [==============================] - 10s 131us/step - loss: 2.3125 - acc: 0.1789\n",
      "Epoch 6/20\n",
      "74645/74645 [==============================] - 10s 129us/step - loss: 2.3018 - acc: 0.1832\n",
      "Epoch 7/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.2931 - acc: 0.1868\n",
      "Epoch 8/20\n",
      "74645/74645 [==============================] - 10s 131us/step - loss: 2.2858 - acc: 0.1895\n",
      "Epoch 9/20\n",
      "74645/74645 [==============================] - 10s 131us/step - loss: 2.2792 - acc: 0.1933\n",
      "Epoch 10/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.2758 - acc: 0.1942\n",
      "Epoch 11/20\n",
      "74645/74645 [==============================] - 11s 143us/step - loss: 2.2704 - acc: 0.1976\n",
      "Epoch 12/20\n",
      "74645/74645 [==============================] - 10s 135us/step - loss: 2.2638 - acc: 0.1988\n",
      "Epoch 13/20\n",
      "74645/74645 [==============================] - 9s 127us/step - loss: 2.2586 - acc: 0.2007\n",
      "Epoch 14/20\n",
      "74645/74645 [==============================] - 9s 126us/step - loss: 2.2555 - acc: 0.2002\n",
      "Epoch 15/20\n",
      "74645/74645 [==============================] - 9s 127us/step - loss: 2.2520 - acc: 0.2034\n",
      "Epoch 16/20\n",
      "74645/74645 [==============================] - 10s 129us/step - loss: 2.2474 - acc: 0.2044\n",
      "Epoch 17/20\n",
      "74645/74645 [==============================] - 10s 129us/step - loss: 2.2442 - acc: 0.2059\n",
      "Epoch 18/20\n",
      "74645/74645 [==============================] - 10s 132us/step - loss: 2.2392 - acc: 0.2084\n",
      "Epoch 19/20\n",
      "74645/74645 [==============================] - 10s 129us/step - loss: 2.2372 - acc: 0.2076\n",
      "Epoch 20/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.2330 - acc: 0.2105\n",
      "Total: Keras-n/e logloss for fold 4 is 2.157345314810835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(40, activation=\"tanh\", kernel_initializer=\"normal\")`\n",
      "/home/matias/kaggle/talking_data/brussels/venv/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"softmax\", kernel_initializer=\"normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic logloss for fold 5 is 2.2819986488647284\n",
      "Epoch 1/20\n",
      "74645/74645 [==============================] - 10s 136us/step - loss: 2.4363 - acc: 0.1276\n",
      "Epoch 2/20\n",
      "74645/74645 [==============================] - 10s 131us/step - loss: 2.3948 - acc: 0.1442\n",
      "Epoch 3/20\n",
      "74645/74645 [==============================] - 10s 132us/step - loss: 2.3557 - acc: 0.1627\n",
      "Epoch 4/20\n",
      "74645/74645 [==============================] - 10s 129us/step - loss: 2.3280 - acc: 0.1727\n",
      "Epoch 5/20\n",
      "74645/74645 [==============================] - 10s 130us/step - loss: 2.3137 - acc: 0.1776\n",
      "Epoch 6/20\n",
      "74645/74645 [==============================] - 10s 128us/step - loss: 2.3042 - acc: 0.1841\n",
      "Epoch 7/20\n",
      "74645/74645 [==============================] - 10s 136us/step - loss: 2.2960 - acc: 0.1860\n",
      "Epoch 8/20\n",
      "74645/74645 [==============================] - 10s 134us/step - loss: 2.2886 - acc: 0.1897\n",
      "Epoch 9/20\n",
      "74645/74645 [==============================] - 10s 131us/step - loss: 2.2814 - acc: 0.1910\n",
      "Epoch 10/20\n",
      "74645/74645 [==============================] - 9s 127us/step - loss: 2.2760 - acc: 0.1950\n",
      "Epoch 11/20\n",
      "74645/74645 [==============================] - 9s 126us/step - loss: 2.2700 - acc: 0.1954\n",
      "Epoch 12/20\n",
      "74645/74645 [==============================] - 9s 125us/step - loss: 2.2666 - acc: 0.1996\n",
      "Epoch 13/20\n",
      "74645/74645 [==============================] - 9s 125us/step - loss: 2.2604 - acc: 0.1992\n",
      "Epoch 14/20\n",
      "74645/74645 [==============================] - 9s 126us/step - loss: 2.2565 - acc: 0.2013\n",
      "Epoch 15/20\n",
      "74645/74645 [==============================] - 9s 126us/step - loss: 2.2535 - acc: 0.2023\n",
      "Epoch 16/20\n",
      "74645/74645 [==============================] - 9s 127us/step - loss: 2.2496 - acc: 0.2014\n",
      "Epoch 17/20\n",
      "74645/74645 [==============================] - 9s 127us/step - loss: 2.2445 - acc: 0.2052\n",
      "Epoch 18/20\n",
      "74645/74645 [==============================] - 10s 127us/step - loss: 2.2404 - acc: 0.2063\n",
      "Epoch 19/20\n",
      "74645/74645 [==============================] - 9s 127us/step - loss: 2.2375 - acc: 0.2081\n",
      "Epoch 20/20\n",
      "74645/74645 [==============================] - 9s 126us/step - loss: 2.2317 - acc: 0.2118\n",
      "Total: Keras-n/e logloss for fold 5 is 2.1510965525920676\n",
      "Keras: logloss for 5 folds is 2.153533206867103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "\n",
    "def baseline_model2(num_columns):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.4, input_shape=(num_columns,)))\n",
    "    model.add(Dense(60))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(40, init=\"normal\", activation=\"tanh\"))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.20))\n",
    "\n",
    "    model.add(Dense(12, init=\"normal\", activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "folds = pd.read_csv(os.path.join(datadir, \"../folds/folds_5.csv\"), index_col=\"device_id\")\n",
    "\n",
    "targetencoder = LabelEncoder().fit(gatrain.group)\n",
    "y = targetencoder.transform(gatrain.group)\n",
    "nclasses = len(targetencoder.classes_)\n",
    "dummy_y = np_utils.to_categorical(y)\n",
    "\n",
    "\n",
    "pred = np.zeros((y.shape[0], nclasses * 2))\n",
    "pred_test = np.zeros((gatest.shape[0], nclasses * 2))\n",
    "n_folds = len(folds[\"fold\"].unique())\n",
    "nbags = 1\n",
    "nepoch = 20\n",
    "print(\n",
    "    \"Starting training Logistic and Keras. Using {} folds, bagged {} times\".format(\n",
    "        n_folds, nbags\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for fold_id in range(1, n_folds + 1):\n",
    "    train_id = folds[\"fold\"].values != fold_id\n",
    "    valid_id = folds[\"fold\"].values == fold_id\n",
    "\n",
    "    Xtr, Ytr = xtrain[train_id, :], y[train_id]\n",
    "    Xva, Yva = xtrain[valid_id, :], y[valid_id]\n",
    "\n",
    "    # Logistic regression\n",
    "    clf1 = LogisticRegression(\n",
    "        C=0.06, multi_class=\"multinomial\", solver=\"lbfgs\"\n",
    "    ) \n",
    "    clf1.fit(Xtr, Ytr)\n",
    "    pred[valid_id, 0:12] = clf1.predict_proba(Xva)\n",
    "\n",
    "    score_val = log_loss(Yva, pred[valid_id, 0:12])\n",
    "    print(\"Logistic logloss for fold {} is {}\".format(fold_id, score_val))\n",
    "\n",
    "    ## Fitting Keras ------------------------>\n",
    "    # First, train on all data, but only no-events feature. Validate with no events:\n",
    "    Xtr, Ytr_dum = xtrain[train_id, :], dummy_y[train_id]\n",
    "    Xva, Yva_dum = xtrain[valid_id, :], dummy_y[valid_id]\n",
    "\n",
    "    for j in range(nbags):\n",
    "        model = baseline_model2(Xtr.shape[1])\n",
    "        fit = model.fit(\n",
    "            xtrain, dummy_y,\n",
    "            epochs=nepoch,\n",
    "            batch_size=512,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        #pred[valid_id_ne, 12:25] += model.predict_generator(\n",
    "        #    generator=batch_generatorp(Xva, 400, False), val_samples=Xva.shape[0]\n",
    "        #)\n",
    "        pred[valid_id, 12:25] += model.predict(Xva)\n",
    "\n",
    "        # pred += model.predict_generator(generator=batch_generatorp(xval, 800, False), val_samples=xval.shape[0])\n",
    "        ## average predictions\n",
    "\n",
    "    pred[valid_id, 12:25] /= nbags\n",
    "\n",
    "    score_val = log_loss(Yva, pred[valid_id, 12:25])\n",
    "    print(\"Total: Keras-n/e logloss for fold {} is {}\".format(fold_id, score_val))\n",
    "\n",
    "score_val = log_loss(y, pred[:, 12:25])\n",
    "print(\"Keras: logloss for {} folds is {}\".format(n_folds, score_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predicting the test set\n",
    "np.random.seed(seed)\n",
    "for j in range(nbags):\n",
    "    model = baseline_model2(xtrain.shape[1])  \n",
    "    fit = model.fit(\n",
    "        xtrain, dummy_y,\n",
    "        epochs=nepoch,\n",
    "        batch_size=381,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    pred_test[test_id, 12:25] += model.predict(\n",
    "        generator=batch_generatorp(xtest[test_id_ne, :], 400, False),\n",
    "        val_samples=xtest[test_id, :].shape[0],\n",
    "    )\n",
    "    print(\"bagg test no events:\", j)\n",
    "\n",
    "pred_test[test_id, 12:25] /= nbags\n",
    "\n",
    "\n",
    "##Predicting the test set Regression- With Events\n",
    "clf2 = LogisticRegression(\n",
    "    C=0.016, multi_class=\"multinomial\", solver=\"lbfgs\"\n",
    ")\n",
    "clf2.fit(Xtrain, y)\n",
    "pred_test[test_id, 0:12] = clf2.predict_proba(Xtest[test_id, :])\n",
    "\n",
    "col_names = np.concatenate((targetencoder.classes_, targetencoder.classes_), axis=0)\n",
    "pred_train_df = pd.DataFrame(pred, index=gatrain.index, columns=col_names)\n",
    "pred_test_df = pd.DataFrame(pred_test, index=gatest.index, columns=col_names)\n",
    "\n",
    "## Generating submissions for kaggle:\n",
    "# pred_train_df.to_csv(\n",
    "#     \"preds/keras_cv5_2_bagging_split_train.csv\", index=True, index_label=\"device_id\"\n",
    "# )\n",
    "# pred_test_df.to_csv(\n",
    "#     \"preds/keras_cv5_2_bagging_split_test.csv\", index=True, index_label=\"device_id\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
